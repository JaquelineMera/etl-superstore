# üìä Temas 
- [Objetivo](#objetivo)
- [Equipo](#equipo)
- [Herramientas](#herramientas)
- [Lenguajes](#lenguajes)
- [Procesamiento y an√°lisis](#procesamiento-y-an√°lisis)
- [An√°lisis exploratorio](#an√°lisis-exploratorio)
- [Resultados](#resultados)
- [Conclusiones](#conclusiones)
- [Recomendaciones](#recomendaciones)
- [Dashboard](#dashboard)

## Objetivo
Dise√±ar e implementar un sistema ETL eficiente que permita a Superstore gestionar grandes vol√∫menes de datos dispersos, optimizando el almacenamiento y la estructura de los datos para facilitar su an√°lisis. A trav√©s de la creaci√≥n de un sistema jer√°rquico con tablas de hechos y dimensiones.

## Equipo
Jaqueline Mera

## Herramientas
- Google BigQuery
- Google Colab
- Google Looker Studio

## Lenguajes
- SQL
- Python

## Procesamiento y an√°lisis
El proceso de los datos, tomo en cuenta cada una de las etapas de Sistema ETL (Extract, Transform y Load)
- Extracci√≥n: esta etapa consiste en recolectar datos de diferentes fuentes, como bases de datos, archivos o APIs.
- Ttransformaci√≥n: implica convertir, limpiar y estructurar los datos seg√∫n las reglas y necesidades del negocio.
- Carga: inserci√≥n de los datos transformados en un sistema de almacenamiento, como un data warehouse.

## Extracci√≥n 
El proceso de extracci√≥n para este proyecto se llev√≥ a cabo a partir de la Flat table de Superstore, que tiene de origen un archivo csv adem√°s se realiz√≥ Web scraping para obtener informaci√≥n de sitios web sobre la competencia de Superstore.
### Web scraping: Buscar datos de otras fuentes
- Se realiz√≥ web scraping para extraer informaci√≥n de sitios web. Este proceso se llev√≥ a cabo en Google Colab utilizando el paquete `Beautiful Soup` de Python. La tabla obtenida es la lista de cadenas de supermercados multinacionales de Wikipedia. [Lista de supermercados](https://en.wikipedia.org/wiki/List_of_supermarket_chains).

## Transformaci√≥n
El proceso de Transformaci√≥n consisti√≥ en el procesamiento y la limpieza de los datos.
### Conectar/importar datos a otras herramientas
- Se han importado los datos de Superstore a una tabla dentro del ambiente de BigQuery.

### Identificar y manejar valores nulos
- Se identificaron valores nulos a trav√©s de comandos SQL, `COUNTIF` y `IS NULL`. No se encontraron nulos.

### Identificar y manejar valores duplicados
- Se han buscado valores duplicados utilizando comandos SQL, `COUNT`, `GROUP BY` y `HAVING`. No se encontraron duplicados en las transacciones (por row). Sin embargo, se identific√≥ que `order_id` se repite en diferentes clientes (`customer_id`), por lo que no se puede considerar `order_id` como identificador √∫nico por orden.

### Identificar y manejar datos discrepantes en variables categ√≥ricas
- Se han buscado datos discrepantes en variables categ√≥ricas utilizando comandos SQL, como `SELECT DISTINCT`. No se encontraron datos discrepantes, y la tabla mantiene congruencia en el formato de las columnas categ√≥ricas.

### Identificar y manejar datos discrepantes en variables num√©ricas
- Se han buscado datos discrepantes en variables num√©ricas utilizando comandos SQL, como `COUNT`, `ARRAY_AGG`, `SAFE_CAST`. No se encontraron datos discrepantes, y la tabla mantiene congruencia en el formato de las columnas num√©ricas (`INTEGER` o `FLOAT`).

### Comprobar y cambiar tipo de dato
- Se cambi√≥ el tipo de dato de las columnas `order_date` y `ship_date` de formato `TIMESTAMP` a formato `DATE`.

## Dise√±o y Carga de la base de datos
A partir del diccionario de datos de Superstore se dise√±√≥ la estructura de la base de datos identificando entidades, atributos y relaciones. Mientras que en el proceso de carga se generaron las tablas de hechos y dimensiones en BigQuery, utilizando comandos SQL como `CREATE OR REPLACE TABLE`.

### Dise√±ar estructura de la base de datos
- El dise√±o de la estructura de la base de datos parti√≥ del diccionario de datos para identificar entidades, atributos y relaciones. El diagrama se construy√≥ en dbdiagram.io.
![superstore](https://github.com/user-attachments/assets/722e13a0-f1f3-417e-ab9e-0ff7c0051102)
*Imagen 1. Propuesta de Diagrama estrella para Superstore. Hecha en dbdiagram.io.*
#### Tabla de hechos (sales_superstore)
- Contiene los datos transaccionales clave para el an√°lisis de ventas en la Superstore, registrando informaci√≥n sobre cada venta realizada. Incluye detalles como el cliente (`customer_id`), el producto (`product_id`), el mercado (`market_id`), la cantidad de productos comprados, el total de ventas, descuentos aplicados, beneficios obtenidos, costos de env√≠o y el a√±o de la transacci√≥n.

#### Dimensi√≥n de √≥rden (dim_order)
- Incluye informaci√≥n detallada sobre los pedidos, como fechas de pedido y env√≠o, modo de env√≠o, ciudad y regi√≥n. Incluye `order_id` y `customer_id`, a partir de los cuales se genera el ID √∫nico de `ticket_id`.

#### Dimensi√≥n de cliente (dim_customer)
- Describe a los clientes, con datos como su nombre y segmento de mercado.

#### Dimensi√≥n de productos (dim_product)
- Proporciona detalles de los productos, como nombre, categor√≠a y subcategor√≠a.

#### Dimensi√≥n de mercado (dim_market)
- Contiene informaci√≥n geogr√°fica de los mercados, incluyendo estado, pa√≠s y mercado asignado.

## Crear estructura de la base de datos
- En el entorno de BigQuery, se generaron las tablas de hechos y dimensiones previamente mencionadas. Se utilizaron comandos SQL como `CREATE OR REPLACE TABLE`. A continuaci√≥n se mencionan los detalles de cada tabla:

- **Tabla dim_order**: La tabla contiene 25,754 √≥rdenes, es decir, eventos de compra. Para esta tabla se construy√≥ el `ticket_id` que se deriva de `order_id` y `customer_id`. Adem√°s, las columnas de `order_date` y `ship_date` cambian de tipo de dato de `TIMESTAMP` a `DATE`.  
- **Tabla dim_customer**: La tabla contiene 4,873 clientes.
- **Tabla dim_product**: La tabla identifica 10,768 productos, que se dividen en 3 categor√≠as y 17 subcategor√≠as.
- **Tabla dim_market**: La tabla identifica 1,126 estados donde se encuentra Superstore. Se construy√≥ el `market_id` a partir de la concatenaci√≥n de estado y pa√≠s.
- **Tabla hechos (sales_superstore)**: La tabla contiene 51,290 transacciones.
- **Tabla competencia**: Contiene 374 compa√±√≠as multinacionales que fungen alrededor del mundo como cadenas de supermercado. Se le asign√≥ un `competence_id`, que es un n√∫mero incremental m√°s el nombre de la compa√±√≠a.

Para ver la construcci√≥n de las tablas de hechos y dimensiones:

## Programar actualizaciones de tabla

#### Estrategia General de Actualizaci√≥n

- **Actualizaci√≥n de tablas de dimensiones primero, hechos despu√©s**: Primero se actualizan las tablas de dimensiones (`dim_order`, `dim_customer`, `dim_product`, `dim_market`), ya que las tablas de hechos dependen de las dimensiones para sus relaciones. Esto asegura que cualquier nueva informaci√≥n o cambio en los datos de referencia (como clientes o productos) est√© disponible antes de cargar los datos en la tabla de hechos.
  
- **Actualizaci√≥n incremental**: Implementar cargas incrementales tanto para las dimensiones como para los hechos. Esto significa que solo se cargar√°n los nuevos registros o aquellos que han cambiado desde la √∫ltima actualizaci√≥n, en lugar de recargar toda la tabla, lo cual es m√°s eficiente en t√©rminos de tiempo y recursos.
  
- **Detecci√≥n de cambios**: Usar t√©cnicas de CDC (Change Data Capture) o marcas de tiempo para detectar cambios y actualizar √∫nicamente los registros modificados. Esto puede implementarse f√°cilmente en BigQuery utilizando consultas basadas en `MERGE` o `INSERT` con `WHERE NOT EXISTS`.

## Esquema de Actualizaci√≥n para cada Tabla

1. **Tabla de Dimensiones: dim_order**  
   - Frecuencia de actualizaci√≥n: Diaria.  
   - Proceso: Cargar nuevos pedidos y actualizar los existentes en base al `ticket_id`.  
   - Estrategia: 
     - Verifica si ya existe un registro con el mismo `ticket_id`.
     - Si no existe, inserta el nuevo registro.
     - Si existe, actualiza los valores.

2. **Tabla de Dimensiones: dim_customer**  
   - Frecuencia de actualizaci√≥n: Diaria.
   - Proceso: Actualizar los datos de los clientes y sus segmentos si se ha modificado la informaci√≥n.
   - Estrategia: Verificar si el `customer_id` ya existe y realizar actualizaciones si es necesario.

3. **Tabla de Dimensiones: dim_product**  
   - Frecuencia de actualizaci√≥n: Semanal o mensual.
   - Proceso: Cargar nuevos productos o actualizaciones de productos existentes.
   - Estrategia: Carga incremental basada en el `product_id`.

4. **Tabla de Dimensiones: dim_market**  
   - Frecuencia de actualizaci√≥n: Semanal o mensual.
   - Proceso: Similar a las otras tablas de dimensiones, se cargan los nuevos mercados o se actualizan los existentes.
   - Estrategia: Carga incremental basada en `market_id`.

5. **Tabla de Hechos: tabla_hechos**  
   - Frecuencia de actualizaci√≥n: Diaria.
   - Proceso: Se debe cargar solo la nueva informaci√≥n de ventas. La actualizaci√≥n debe incluir ventas adicionales o correcciones a las transacciones pasadas.
   - Estrategia: Carga incremental basada en el `ticket_id` para insertar nuevas ventas y actualizar las existentes.

### Secuencia recomendada de actualizaci√≥n:
1. Actualizar `dim_market` (si hay cambios en los mercados).
2. Actualizar `dim_customer` (para incluir nuevos clientes).
3. Actualizar `dim_product` (para nuevos productos o categor√≠as).
4. Actualizar `dim_order` (para los nuevos pedidos).
5. Actualizar `tabla_hechos` (las transacciones o ventas).

Este proceso garantiza que las dimensiones est√©n actualizadas antes de que se realicen las nuevas inserciones en la tabla de hechos, manteniendo la integridad referencial.

## Propuestas de Pipelines para la actualizaci√≥n de datos
Se proponen dos pipelines para la actualizaci√≥n de datos en Superstore.
Se proponen un pipeline en GCP, es decir con los servicios Google Cloud Plataform y un pipeline Open Sources. Ambos pipelines incluyen etapas de ingesta de donde se extraer√° la informaci√≥n, una etapa de procesamiento, donde se transformar√° la data, y por √∫ltimo una etapa de carga donde se llevar√° a cabo el modelado de datos, est√° propuesta de pipeline tambi√©n incluye la visualizaci√≥n, orquestaci√≥n y automatizaci√≥n del mismo pipeline. 
- **Pipeline en GCP (Google Cloud Platform)**:
  - Cloud Storage (Ingesta de datos crudos) ‚Üí Dataproc (Procesamiento de datos) ‚Üí BigQuery (Modelado de hechos y dimensiones) ‚Üí Looker Studio (Visualizaci√≥n) ‚Üí Cloud Composer (Orquestaci√≥n/Automatizaci√≥n).
  
- **Pipeline Open Source**:
  - Hadoop HDFS (Ingesta de datos crudos) ‚Üí Apache Spark (Procesamiento) ‚Üí Hive (Modelado de hechos y dimensiones) ‚Üí Tableau* (Visualizaci√≥n) ‚Üí Airflow (Orquestaci√≥n/Automatizaci√≥n).
![Pipeline_GCP_OS](https://github.com/user-attachments/assets/57ab176f-bc96-4dad-b82c-aed51f2bf260)
*Imagen 2. Propuesta de Pipeline. Hecha en Lucidchart.*

**Nota**: La automatizaci√≥n con Cloud Composer/Apache Airflow debe orquestar la actualizaci√≥n de cada tabla en el orden correcto.

### Unir tablas
Se cre√≥ una tabla para el an√°lisis exploratorio a partir de las tablas de hechos y dimensiones utilizando comandos SQL, como `LEFT JOIN`.

## An√°lisis exploratorio

- **Agrupar datos**: Se agruparon los datos seg√∫n variables categ√≥ricas (Ventas y Ganancias) para identificar patrones y tendencias.
- **Visualizar las variables categ√≥ricas**: Se crearon gr√°ficos (Ventas y Ganancias, por tipo de cliente, categor√≠a, mercado y productos)para obtener una mejor comprensi√≥n de los datos.
- **Aplicar medidas de tendencia central y dispersi√≥n**: Se calcularon estad√≠sticas como la media, mediana, rango y desviaci√≥n est√°ndar para Ventas, Ganancias, Costos de env√≠o y Descuentos.
- **Visualizar distribuci√≥n**: Se crearon diagramas de dispersi√≥n (Ventas vs Ganancias vs Costo de env√≠o).
- **Visualizar el comportamiento de los datos a lo largo del tiempo**: Se crearon gr√°ficos para observar datos a trav√©s de los a√±os (Ventas y Ganancias).

**Nota**: Para observar el an√°lisis exploratorio, revisar el Dashboard en los apartados EDA.

## Resultados

### Resultados Superstore
- **Ventas globales**:
  - Total de ventas: $12.6 millones con una ganancia de $1.5 millones, representando un margen de ganancia aproximado del 11.9%.
  - Transacciones: Se han registrado 51,290 transacciones con un ticket promedio de $245.88 y un promedio de 7 unidades vendidas por ticket (basado en 178,312 productos vendidos).

- **Segmentaci√≥n**:
  - **Consumer**: Representa el mayor segmento, con el 51.7% de las ventas ($6.5 millones) y ganancias de $749.2 mil.
  - **Corporate**: Contribuye con el 30.1% de las ventas ($3.8 millones) y $441.2 mil de ganancias.
  - **Home Office**: Menor participaci√≥n (18.2%) con $2.3 millones de ventas y $227 mil de ganancias.

- **Evoluci√≥n temporal**:
  - Crecimiento constante: Las ventas aumentaron a√±o tras a√±o desde 2011 ($2.3M) hasta 2014 ($4.3M). Las ganancias tambi√©n crecieron proporcionalmente, mostrando un buen control sobre los costos.
  - Costos de env√≠o: Los costos de env√≠o han crecido un 88% entre 2011 ($244.3K) y 2014 ($460.5K), lo que podr√≠a estar impactando las ganancias si no se gestionan adecuadamente.

- **Ventas por regi√≥n**:
  - **APAC** lidera con $3.6 millones en ventas y $437.5 mil en ganancias, seguido por la **UE** y **EEUU**.
  - **LATAM** se destaca con $2.2 millones de ventas, pero sus ganancias ($221.6 mil) son m√°s bajas que otras regiones, lo que podr√≠a sugerir m√°rgenes menores.
  - **Canad√°** es el mercado m√°s peque√±o con $66.9 mil en ventas y $17.8 mil de ganancias.

- **Ventas por categor√≠a**:
  - **Technology** es la categor√≠a con m√°s ventas ($4.7 millones) y ganancias ($663.8 mil), mostrando su popularidad y rentabilidad.
  - **Office Supplies** y **Furniture** tambi√©n son fuertes en ventas y ganancias, aunque **Furniture** tiene un margen de ganancia m√°s bajo.

- **Subcategor√≠as clave**:
  - **Phones** y **Copiers** son las subcategor√≠as m√°s vendidas, y **Copiers** tiene el mayor margen de ganancia ($258K).
  - **Chairs** tambi√©n es una subcategor√≠a destacada en ventas, pero su margen de ganancia es menor comparado con **Copiers** y **Phones**.

### Resultados del Pipeline de Actualizaci√≥n
En el proyecto de ETL para Superstore, se ha implementado un pipeline s√≥lido que integra la ingesta, transformaci√≥n y almacenamiento de datos con el uso eficiente de BigQuery para el manejo de tablas de hechos y dimensiones. El pipeline permite una actualizaci√≥n eficiente, utilizando cargas incrementales y procesos automatizados, minimizando la carga computacional y asegurando que solo los datos nuevos o modificados sean procesados.

## Conclusiones

### Conclusiones Superstore
- **Segmento Consumer como motor principal**: Este segmento genera m√°s de la mitad de las ventas y ganancias, pero la proporci√≥n de ganancias (11.5%) es algo menor que en los otros segmentos, lo que sugiere oportunidades de mejora en eficiencia o precios.
- **Crecimiento s√≥lido pero con aumento de costos**: El crecimiento de las ventas ha sido consistente, pero los costos de env√≠o han aumentado significativamente. Se debe investigar c√≥mo optimizar estos costos para mantener m√°rgenes saludables.
- **Disparidades regionales**: APAC y la UE son los mercados m√°s rentables, mientras que LATAM muestra un margen m√°s bajo. Podr√≠a ser √∫til investigar factores locales que afectan las ganancias en LATAM.
- **Categor√≠as tecnol√≥gicas dominantes**: Los productos de tecnolog√≠a no solo son los m√°s vendidos, sino que tambi√©n tienen un margen alto. Sin embargo, otras categor√≠as como muebles, aunque generan muchas ventas, tienen m√°rgenes m√°s bajos.

### Conclusi√≥n del Pipeline de Actualizaci√≥n
El pipeline asegura la integridad referencial al actualizar las dimensiones antes que los hechos, optimizando el rendimiento del proceso de carga. El dise√±o de cargas incrementales y automatizaci√≥n minimiza la carga computacional, asegurando que los datos est√©n listos para su an√°lisis.

## Recomendaciones

### Recomendaciones Superstore
- **Optimizaci√≥n de costos de env√≠o**: Considerar negociaciones con proveedores de log√≠stica o la implementaci√≥n de estrategias de distribuci√≥n m√°s eficientes para reducir los costos de env√≠o que han crecido considerablemente.
- **Revisar precios en LATAM**: Dado que las ventas son fuertes, pero las ganancias son relativamente bajas, podr√≠a ser recomendable ajustar los precios o reducir costos en LATAM para mejorar el margen.
- **Enfoque en subcategor√≠as rentables**: Fortalecer las estrategias de ventas en subcategor√≠as con altos m√°rgenes, como **Copiers** y **Phones**, ya que ofrecen los mayores beneficios. Al mismo tiempo, se debe evaluar c√≥mo mejorar el margen en otras subcategor√≠as como **Chairs**.
- **Estrategias de fidelizaci√≥n en el segmento Consumer**: Dado que este segmento es el m√°s grande, se puede considerar la implementaci√≥n de programas de fidelizaci√≥n o personalizaci√≥n de ofertas para mantener el crecimiento de ventas y aumentar los m√°rgenes de ganancia.

### Recomendaciones Pipeline
- **Automatizaci√≥n Completa del Pipeline**: Considera implementar Cloud Composer (Airflow) para la orquestaci√≥n autom√°tica de todas las etapas del pipeline, programando actualizaciones diarias, semanales o mensuales seg√∫n sea necesario.
- **Mejorar el Rendimiento con Particionamiento y Clustering**: Aprovecha el particionamiento en BigQuery, particionando la tabla de hechos por a√±o o fechas de transacci√≥n para mejorar el rendimiento de consultas sobre grandes vol√∫menes de datos.
- **Monitoreo y Alertas**: Implementa un sistema de monitoreo y alertas para asegurar que las cargas y transformaciones de datos se ejecuten correctamente. Herramientas como Google Cloud Monitoring pueden detectar fallos en el pipeline y enviarte notificaciones.
- **Incrementar la Capacidad de Visualizaci√≥n**: Considera expandir el uso de Looker Studio o herramientas adicionales como Tableau si los an√°lisis requieren visualizaciones m√°s complejas.
- **Escalabilidad del Pipeline**: A medida que los vol√∫menes de datos crezcan, puedes explorar el uso de Dataflow para el procesamiento en streaming, lo que te permitir√° manejar datos en tiempo real, adaptando el pipeline para an√°lisis en vivo.
- **Optimizaci√≥n del Almacenamiento**: Revisa peri√≥dicamente las tablas de hechos y dimensiones para eliminar registros innecesarios o consolidar datos hist√≥ricos, evitando que los costos de almacenamiento en BigQuery crezcan desproporcionadamente.

## Dashboard
[Ventas-Superstore](https://lookerstudio.google.com/reporting/031a155d-6e2f-442c-994d-9a260126ade3)

