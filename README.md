# üìä Temas 
- [Objetivo](#objetivo)
- [Equipo](#equipo)
- [Herramientas](#herramientas)
- [Lenguajes](#lenguajes)
- [Procesamiento y an√°lisis](#procesamiento-y-an√°lisis)
- [An√°lisis exploratorio](#an√°lisis-exploratorio)
- [Resultados](#resultados)
- [Conclusiones](#conclusiones)
- [Recomendaciones](#recomendaciones)
- [Enlaces](#enlaces)

## Objetivo
Dise√±ar e implementar un sistema ETL eficiente que permita a Superstore gestionar grandes vol√∫menes de datos dispersos, optimizando el almacenamiento y la estructura de los datos para facilitar su an√°lisis. A trav√©s de la creaci√≥n de un sistema jer√°rquico con tablas de hechos y dimensiones.

## Equipo
Jaqueline Mera

## Herramientas
- Google BigQuery
- Google Colab
- Google Looker Studio

## Lenguajes
- SQL
- Python

## Procesamiento y an√°lisis

### Conectar/importar datos a otras herramientas
- Se han importado los datos de Superstore a una tabla dentro del ambiente de BigQuery.

### Identificar y manejar valores nulos
- Se identificaron valores nulos a trav√©s de comandos SQL, `COUNTIF` y `IS NULL`. No se encontraron nulos.

### Identificar y manejar valores duplicados
- Se han buscado valores duplicados utilizando comandos SQL, `COUNT`, `GROUP BY` y `HAVING`. No se encontraron duplicados en las transacciones (por row). Sin embargo, se identific√≥ que `order_id` se repite en diferentes clientes (`customer_id`), por lo que no se puede considerar `order_id` como identificador √∫nico por orden.

### Identificar y manejar datos discrepantes en variables categ√≥ricas
- Se han buscado datos discrepantes en variables categ√≥ricas utilizando comandos SQL, como `SELECT DISTINCT`. No se encontraron datos discrepantes, y la tabla mantiene congruencia en el formato de las columnas categ√≥ricas.

### Identificar y manejar datos discrepantes en variables num√©ricas
- Se han buscado datos discrepantes en variables num√©ricas utilizando comandos SQL, como `COUNT`, `ARRAY_AGG`, `SAFE_CAST`. No se encontraron datos discrepantes, y la tabla mantiene congruencia en el formato de las columnas num√©ricas (`INTEGER` o `FLOAT`).

### Comprobar y cambiar tipo de dato
- Se cambi√≥ el tipo de dato de las columnas `order_date` y `ship_date` de formato `TIMESTAMP` a formato `DATE`.

### Buscar datos de otras fuentes
- Se realiz√≥ web scraping para extraer informaci√≥n de sitios web. Este proceso se llev√≥ a cabo en Google Colab utilizando el paquete `Beautiful Soup` de Python. La tabla obtenida es la lista de cadenas de supermercados multinacionales de Wikipedia. [Lista de supermercados](https://en.wikipedia.org/wiki/List_of_supermarket_chains).

### Dise√±ar estructura de la base de datos
- El dise√±o de la estructura de la base de datos parti√≥ del diccionario de datos para identificar entidades, atributos y relaciones. El diagrama se construy√≥ en dbdiagram.io.

#### Tabla de hechos (sales_superstore)
- Contiene los datos transaccionales clave para el an√°lisis de ventas en la Superstore, registrando informaci√≥n sobre cada venta realizada. Incluye detalles como el cliente (`customer_id`), el producto (`product_id`), el mercado (`market_id`), la cantidad de productos comprados, el total de ventas, descuentos aplicados, beneficios obtenidos, costos de env√≠o y el a√±o de la transacci√≥n.

#### Dimensi√≥n de √≥rden (dim_order)
- Incluye informaci√≥n detallada sobre los pedidos, como fechas de pedido y env√≠o, modo de env√≠o, ciudad y regi√≥n. Incluye `order_id` y `customer_id`, a partir de los cuales se genera el ID √∫nico de `ticket_id`.

#### Dimensi√≥n de cliente (dim_customer)
- Describe a los clientes, con datos como su nombre y segmento de mercado.

#### Dimensi√≥n de productos (dim_product)
- Proporciona detalles de los productos, como nombre, categor√≠a y subcategor√≠a.

#### Dimensi√≥n de mercado (dim_market)
- Contiene informaci√≥n geogr√°fica de los mercados, incluyendo estado, pa√≠s y mercado asignado.

## Crear estructura de la base de datos
- En el entorno de BigQuery, se generaron las tablas de hechos y dimensiones previamente mencionadas. Se utilizaron comandos SQL como `CREATE OR REPLACE TABLE`. A continuaci√≥n se mencionan los detalles de cada tabla:

- **Tabla dim_order**: La tabla contiene 25,754 √≥rdenes, es decir, eventos de compra. Para esta tabla se construy√≥ el `ticket_id` que se deriva de `order_id` y `customer_id`. Adem√°s, las columnas de `order_date` y `ship_date` cambian de tipo de dato de `TIMESTAMP` a `DATE`.  
- **Tabla dim_customer**: La tabla contiene 4,873 clientes.
- **Tabla dim_product**: La tabla identifica 10,768 productos, que se dividen en 3 categor√≠as y 17 subcategor√≠as.
- **Tabla dim_market**: La tabla identifica 1,126 estados donde se encuentra Superstore. Se construy√≥ el `market_id` a partir de la concatenaci√≥n de estado y pa√≠s.
- **Tabla hechos (sales_superstore)**: La tabla contiene 51,290 transacciones.
- **Tabla competencia**: Contiene 374 compa√±√≠as multinacionales que fungen alrededor del mundo como cadenas de supermercado. Se le asign√≥ un `competence_id`, que es un n√∫mero incremental m√°s el nombre de la compa√±√≠a.

Para ver la construcci√≥n de las tablas de hechos y dimensiones:

## Programar actualizaciones de tabla

#### Estrategia General de Actualizaci√≥n

- **Actualizaci√≥n de tablas de dimensiones primero, hechos despu√©s**: Primero se actualizan las tablas de dimensiones (`dim_order`, `dim_customer`, `dim_product`, `dim_market`), ya que las tablas de hechos dependen de las dimensiones para sus relaciones. Esto asegura que cualquier nueva informaci√≥n o cambio en los datos de referencia (como clientes o productos) est√© disponible antes de cargar los datos en la tabla de hechos.
  
- **Actualizaci√≥n incremental**: Implementar cargas incrementales tanto para las dimensiones como para los hechos. Esto significa que solo se cargar√°n los nuevos registros o aquellos que han cambiado desde la √∫ltima actualizaci√≥n, en lugar de recargar toda la tabla, lo cual es m√°s eficiente en t√©rminos de tiempo y recursos.
  
- **Detecci√≥n de cambios**: Usar t√©cnicas de CDC (Change Data Capture) o marcas de tiempo para detectar cambios y actualizar √∫nicamente los registros modificados. Esto puede implementarse f√°cilmente en BigQuery utilizando consultas basadas en `MERGE` o `INSERT` con `WHERE NOT EXISTS`.

## Esquema de Actualizaci√≥n para cada Tabla

1. **Tabla de Dimensiones: dim_order**  
   - Frecuencia de actualizaci√≥n: Diaria.  
   - Proceso: Cargar nuevos pedidos y actualizar los existentes en base al `ticket_id`.  
   - Estrategia: 
     - Verifica si ya existe un registro con el mismo `ticket_id`.
     - Si no existe, inserta el nuevo registro.
     - Si existe, actualiza los valores.

2. **Tabla de Dimensiones: dim_customer**  
   - Frecuencia de actualizaci√≥n: Diaria.
   - Proceso: Actualizar los datos de los clientes y sus segmentos si se ha modificado la informaci√≥n.
   - Estrategia: Verificar si el `customer_id` ya existe y realizar actualizaciones si es necesario.

3. **Tabla de Dimensiones: dim_product**  
   - Frecuencia de actualizaci√≥n: Semanal o mensual.
   - Proceso: Cargar nuevos productos o actualizaciones de productos existentes.
   - Estrategia: Carga incremental basada en el `product_id`.

4. **Tabla de Dimensiones: dim_market**  
   - Frecuencia de actualizaci√≥n: Semanal o mensual.
   - Proceso: Similar a las otras tablas de dimensiones, se cargan los nuevos mercados o se actualizan los existentes.
   - Estrategia: Carga incremental basada en `market_id`.

5. **Tabla de Hechos: tabla_hechos**  
   - Frecuencia de actualizaci√≥n: Diaria.
   - Proceso: Se debe cargar solo la nueva informaci√≥n de ventas. La actualizaci√≥n debe incluir ventas adicionales o correcciones a las transacciones pasadas.
   - Estrategia: Carga incremental basada en el `ticket_id` para insertar nuevas ventas y actualizar las existentes.

### Secuencia recomendada de actualizaci√≥n:
1. Actualizar `dim_market` (si hay cambios en los mercados).
2. Actualizar `dim_customer` (para incluir nuevos clientes).
3. Actualizar `dim_product` (para nuevos productos o categor√≠as).
4. Actualizar `dim_order` (para los nuevos pedidos).
5. Actualizar `tabla_hechos` (las transacciones o ventas).

Este proceso garantiza que las dimensiones est√©n actualizadas antes de que se realicen las nuevas inserciones en la tabla de hechos, manteniendo la integridad referencial.

## Propuesta de Pipelines para la actualizaci√≥n de datos
- **Pipeline en GCP (Google Cloud Platform)**:
  - Cloud Storage (Ingesta de datos crudos) ‚Üí Dataproc (Procesamiento de datos) ‚Üí BigQuery (Modelado de hechos y dimensiones) ‚Üí Looker Studio (Visualizaci√≥n) ‚Üí Cloud Composer (Orquestaci√≥n/Automatizaci√≥n).
  
- **Pipeline Open Source**:
  - Hadoop HDFS (Ingesta de datos crudos) ‚Üí Apache Spark (Procesamiento) ‚Üí Hive (Modelado de hechos y dimensiones) ‚Üí Tableau* (Visualizaci√≥n) ‚Üí Airflow (Orquestaci√≥n/Automatizaci√≥n).

*Imagen 2. Propuesta de Pipeline. Hecha en Lucidchart.*

**Nota**: La automatizaci√≥n con Cloud Composer/Apache Airflow debe orquestar la actualizaci√≥n de cada tabla en el orden correcto.

### Unir tablas
Se cre√≥ una tabla para el an√°lisis exploratorio a partir de las tablas de hechos y dimensiones utilizando comandos SQL, como `LEFT JOIN`.

